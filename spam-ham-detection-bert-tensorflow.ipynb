{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43d77a84-03e6-444c-ae8e-c6ec3fb29c61",
   "metadata": {},
   "source": [
    "# **Spam Ham Detection Using BERT and Tensorflow**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f758b9b-dab7-4279-929a-489595f95945",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0124063-db8f-4910-a79b-bc86ed6b348b",
   "metadata": {},
   "source": [
    "##### <u>**Project Summary**</u>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ddf71ae5-c10b-4ad5-9651-b5ccd899d8bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37e9cdff-2369-4ff8-8ef2-2028018b55e8",
   "metadata": {},
   "source": [
    "##### <u>**GitHub Link**</u>\n",
    "[Click Here](https://github.com/ajitmane36/spam-ham-detection-bert-tensorflow.git)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa6cc20-607f-4599-8f22-b0e6ee0c306d",
   "metadata": {},
   "source": [
    "##### <u>**Problem Statement**</u>\n",
    "\n",
    "- The data is related to the classification of emails into spam or ham (non-spam). The goal of this project is to develop a model using BERT and TensorFlow to predict whether an email is spam or not based on its content. By fine-tuning a pre-trained BERT model, the objective is to enhance the accuracy and efficiency of email classification, ensuring that legitimate emails are delivered to the inbox while spam is effectively filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d4c68e-068a-42d1-9392-2532e4ffb3be",
   "metadata": {},
   "source": [
    "##### <u>**Data Description**</u>\n",
    "\n",
    "- **Message**: Description of the email content (text).\n",
    "- **Category**: Indicates whether the email is spam (1) or not (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95b8893b-cd6c-499e-8ec1-ed027fe378b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow_text (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow_text\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c76b328-bd1c-4f62-8797-cce75b73cd44",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtext\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# filter warnings\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Importing tensorflow libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "\n",
    "# filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc56a225-5626-46a6-8291-da92f03ce470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Loading\n",
    "df=pd.read_csv(r\"C:\\Users\\ajitm\\Downloads\\DS Projects\\Deep Larning Projects\\1. Text Classification Using BERT & Tensorflow\\spam_emails_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd050d32-6a75-466c-b1eb-c0acd0423039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fist five observations\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3326d4cf-66bd-4eb4-a9e1-477595844a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last five observations\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904d6f01-66ed-4db9-ad02-385d7e78adf9",
   "metadata": {},
   "source": [
    "##### <u>**Data Inispection**</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f26671-51b5-48e1-b1ab-46b5f6f5bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of dataset\n",
    "df.shape\n",
    "print(f'Dataset has {df.shape[0]} observations and {df.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f9ad0c-2dd5-4876-bf9d-312fad23023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset columns\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fff068-7dfe-4986-815e-ce152790c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information of dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651b008e-65d7-42cf-b2d9-09e4f25ac81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic description of dataset\n",
    "df.groupby('Category').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c09b38f-796e-4b6e-8941-f69603917706",
   "metadata": {},
   "source": [
    "- Dataset having 4825 Ham observations and 747 spam observations.\n",
    "- Class imbalance seen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836b06c1-5cbc-4258-b463-a25978b50a46",
   "metadata": {},
   "source": [
    "##### <u>**Data Wrangling**</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e46691-ad67-4dbe-857c-497f7719cf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cehcking  duplicates value in each feature\n",
    "duplicates_df=pd.DataFrame({'columns':df.columns, 'number_of_duplicates': df.duplicated().sum()}).sort_values(by='number_of_duplicates', ascending=False)\n",
    "print(duplicates_df)\n",
    "print(f'Dataset having {df.duplicated().sum()} duplicates values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00833df7-b353-47ce-b761-e2a7c3695c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e265195-9c0e-4f04-8ae5-ab58c159c71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values\n",
    "null_df=pd.DataFrame({'columns': df.columns, 'num_of_nulls': df.isna().sum()})\n",
    "print(null_df )\n",
    "print(f'Dataset have {df.isna().sum()} null values.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807666e8-c4d6-423a-820a-981d07bcf809",
   "metadata": {},
   "source": [
    "##### <u>**Exploratory Data Analysis**</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fc7acd-0dd1-4c57-8c4a-641c054e8198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min and Max length of email\n",
    "print(\"Smallest email\")\n",
    "print('__'*50)\n",
    "print(df.Message.min())\n",
    "print(f'Length: {len(df.Message.min())}')\n",
    "print('=='*50)\n",
    "print(\"Largest email\")\n",
    "print('__'*50)\n",
    "print(df.Message.max())\n",
    "print(f'Length: {len(df.Message.max())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53025be-70d9-4f49-a942-fa5258aa87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to Notate the percent count of each value on the bars\n",
    "def annot_percent(axes):\n",
    "    '''Takes axes as input and labels the percent count of each bar in a countplot'''\n",
    "    for p in plot.patches:\n",
    "        total = sum(p.get_height() for p in plot.patches)/100\n",
    "        percent = round((p.get_height()/total),2)\n",
    "        x = p.get_x() + p.get_width()/2\n",
    "        y = p.get_height()\n",
    "        plot.annotate(f'{percent}%', (x, y), ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef0e9bd-a7da-404e-8e3b-dd0ab31032a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for diffrent categories of email \n",
    "plt.figure(figsize=(7,5))\n",
    "plot=plt.subplot(111)\n",
    "ax=sns.countplot(x=df.Category, hue=df.Category)\n",
    "ax.set_title('Countplot for different emails categories')\n",
    "ax.legend('Category', title='Emails Categories')\n",
    "annot_percent(plot)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd7c082-7465-4912-be62-8e53c612c4fb",
   "metadata": {},
   "source": [
    "- Dataset have 12.43% of spam and 87.57% ham observations.\n",
    "- Class imbalnce seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d67cf7c-e586-4cb5-9002-fb13b4132297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of each emial\n",
    "df['length']=df['Message'].apply(lambda x:len(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5d4e60-161d-4e54-a478-8eb2571170f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram for length of emails\n",
    "plt.figure(figsize=(7,5))\n",
    "ax=sns.histplot(x=df.length, hue=df.Category)\n",
    "ax.set_title('Histogram for length of emails')\n",
    "ax.axvline(df.length.mean(), color='r', linestyle='--')\n",
    "ax.text(df.length.mean(), ax.get_ylim()[1]*0.9, f'Average Emails Length: {df.length.mean():.2f}', \n",
    "        color='red', ha='left', va='top', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb74227-cd2a-4c38-b772-759a960ffd9c",
   "metadata": {},
   "source": [
    "- The average email length seen is 79.\n",
    "- The majority of emails are shorter than 200 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8793771-2065-4976-b344-7a32cebb6849",
   "metadata": {},
   "source": [
    "##### <u>**Feature Engineering & Data Pre-processing**</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a24bbb-2e03-493d-95c7-b5b7f0829c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset look\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2be276d-a911-4be6-ad8e-a022d72cbf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary 'length' feature\n",
    "df.drop(columns='length', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856b98eb-29f3-4d51-b2b6-866ec8a064f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check category in 'Category' feature\n",
    "df.Category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08899cef-6143-4b06-8b7f-23ce32864c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical encoding for 'Category' feature\n",
    "df['spam']=df['Category'].apply(lambda x: 1 if x=='spam' else 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55d5d10-e680-4375-8abb-c42be69fbcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating dependant and independant varibale\n",
    "\n",
    "# Independant feature\n",
    "X=df['Message'].copy()\n",
    "# Dependant feature\n",
    "y=df['spam'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e102c-8ad3-4b7a-86f5-4c4dfe864aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aa5f69-62f0-428e-801c-37020e679179",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Shape of splits\n",
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'X_test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "929ee1b0-1e87-4b3b-babd-65fc58fbd9b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Op type not registered 'CaseFoldUTF8' in binary running on AJITS-PC. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib (e.g. `tf.contrib.resampler`), accessing should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3061\u001b[0m, in \u001b[0;36mGraph.op_def_for_type\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m   3060\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3061\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op_def_cache[\u001b[38;5;28mtype\u001b[39m]\n\u001b[0;32m   3062\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'CaseFoldUTF8'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Download BERT preprocessing and encoder model from tensorflow hub\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m hub\u001b[38;5;241m.\u001b[39mKerasLayer(\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-preprocess/3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m encoder \u001b[38;5;241m=\u001b[39m hub\u001b[38;5;241m.\u001b[39mKerasLayer(\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.kaggle.com/models/tensorflow/bert/TensorFlow2/bert-en-uncased-l-12-h-768-a-12/2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\keras_layer.py:165\u001b[0m, in \u001b[0;36mKerasLayer.__init__\u001b[1;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_shape \u001b[38;5;241m=\u001b[39m data_structures\u001b[38;5;241m.\u001b[39mNoDependency(\n\u001b[0;32m    162\u001b[0m       _convert_nest_to_shapes(output_shape))\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_options \u001b[38;5;241m=\u001b[39m load_options\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func \u001b[38;5;241m=\u001b[39m load_module(handle, tags, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_options)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_hub_module_v1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_hub_module_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Update with the defaults when using legacy TF1 Hub format.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\keras_layer.py:467\u001b[0m, in \u001b[0;36mload_module\u001b[1;34m(handle, tags, load_options)\u001b[0m\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:  \u001b[38;5;66;03m# Expected before TF2.4.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m       set_load_options \u001b[38;5;241m=\u001b[39m load_options\n\u001b[1;32m--> 467\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module_v2\u001b[38;5;241m.\u001b[39mload(handle, tags\u001b[38;5;241m=\u001b[39mtags, options\u001b[38;5;241m=\u001b[39mset_load_options)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126\u001b[0m, in \u001b[0;36mload\u001b[1;34m(handle, tags, options)\u001b[0m\n\u001b[0;32m    123\u001b[0m   obj \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mload_v2(\n\u001b[0;32m    124\u001b[0m       module_path, tags\u001b[38;5;241m=\u001b[39mtags, options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m   obj \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mload_v2(module_path, tags\u001b[38;5;241m=\u001b[39mtags)\n\u001b[0;32m    127\u001b[0m obj\u001b[38;5;241m.\u001b[39m_is_hub_module_v1 \u001b[38;5;241m=\u001b[39m is_hub_module_v1  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:912\u001b[0m, in \u001b[0;36mload\u001b[1;34m(export_dir, tags, options)\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(export_dir, os\u001b[38;5;241m.\u001b[39mPathLike):\n\u001b[0;32m    911\u001b[0m   export_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(export_dir)\n\u001b[1;32m--> 912\u001b[0m result \u001b[38;5;241m=\u001b[39m load_partial(export_dir, \u001b[38;5;28;01mNone\u001b[39;00m, tags, options)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:1042\u001b[0m, in \u001b[0;36mload_partial\u001b[1;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minit_scope():\n\u001b[0;32m   1041\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1042\u001b[0m     loader \u001b[38;5;241m=\u001b[39m Loader(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0;32m   1043\u001b[0m                     ckpt_options, options, filters)\n\u001b[0;32m   1044\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m   1046\u001b[0m         \u001b[38;5;28mstr\u001b[39m(err) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m You may be trying to load on a different device \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1047\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom the computational device. Consider setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1048\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1049\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto the io_device such as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/job:localhost\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:161\u001b[0m, in \u001b[0;36mLoader.__init__\u001b[1;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proto \u001b[38;5;241m=\u001b[39m object_graph_proto\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_dir \u001b[38;5;241m=\u001b[39m export_dir\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_functions \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 161\u001b[0m     function_deserialization\u001b[38;5;241m.\u001b[39mload_function_def_library(\n\u001b[0;32m    162\u001b[0m         library\u001b[38;5;241m=\u001b[39mmeta_graph\u001b[38;5;241m.\u001b[39mgraph_def\u001b[38;5;241m.\u001b[39mlibrary,\n\u001b[0;32m    163\u001b[0m         saved_object_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proto,\n\u001b[0;32m    164\u001b[0m         wrapper_function\u001b[38;5;241m=\u001b[39m_WrapperFunction))\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# Store a set of all concrete functions that have been set up with\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# captures.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restored_concrete_functions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py:456\u001b[0m, in \u001b[0;36mload_function_def_library\u001b[1;34m(library, saved_object_graph, load_shared_name_suffix, wrapper_function)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# There is no need to copy all functions into the function def graph. It\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# leads to a O(n^2) increase of memory when importing functions and the\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# extra function definitions are a no-op since they already imported as a\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# function before and passed in explicitly (due to the topologic sort\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# import).\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[1;32m--> 456\u001b[0m   func_graph \u001b[38;5;241m=\u001b[39m function_def_lib\u001b[38;5;241m.\u001b[39mfunction_def_to_graph(\n\u001b[0;32m    457\u001b[0m       fdef,\n\u001b[0;32m    458\u001b[0m       structured_input_signature\u001b[38;5;241m=\u001b[39mstructured_input_signature,\n\u001b[0;32m    459\u001b[0m       structured_outputs\u001b[38;5;241m=\u001b[39mstructured_outputs)\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# Restores gradients for function-call ops (not the same as ops that use\u001b[39;00m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;66;03m# custom gradients)\u001b[39;00m\n\u001b[0;32m    462\u001b[0m _restore_gradient_functions(func_graph, renamed_functions, loaded_gradients)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\function_def_to_graph.py:91\u001b[0m, in \u001b[0;36mfunction_def_to_graph\u001b[1;34m(fdef, structured_input_signature, structured_outputs, input_shapes, propagate_device_spec, include_library_functions)\u001b[0m\n\u001b[0;32m     88\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m         input_shapes\u001b[38;5;241m.\u001b[39mappend(input_shape)\n\u001b[1;32m---> 91\u001b[0m graph_def, nested_to_flat_tensor_name \u001b[38;5;241m=\u001b[39m function_def_to_graph_def(\n\u001b[0;32m     92\u001b[0m     fdef, input_shapes, include_library_functions\u001b[38;5;241m=\u001b[39minclude_library_functions\n\u001b[0;32m     93\u001b[0m )\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m func_graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m     96\u001b[0m   \u001b[38;5;66;03m# Add all function nodes to the graph.\u001b[39;00m\n\u001b[0;32m     97\u001b[0m   importer\u001b[38;5;241m.\u001b[39mimport_graph_def_for_function(\n\u001b[0;32m     98\u001b[0m       graph_def, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, propagate_device_spec\u001b[38;5;241m=\u001b[39mpropagate_device_spec)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\function_def_to_graph.py:330\u001b[0m, in \u001b[0;36mfunction_def_to_graph_def\u001b[1;34m(fdef, input_shapes, include_library_functions)\u001b[0m\n\u001b[0;32m    328\u001b[0m       graph_def\u001b[38;5;241m.\u001b[39mlibrary\u001b[38;5;241m.\u001b[39mgradient\u001b[38;5;241m.\u001b[39mextend([grad_def])\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 330\u001b[0m   op_def \u001b[38;5;241m=\u001b[39m default_graph\u001b[38;5;241m.\u001b[39mop_def_for_type(node_def\u001b[38;5;241m.\u001b[39mop)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m op_def\u001b[38;5;241m.\u001b[39mattr:\n\u001b[0;32m    333\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m attr\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3064\u001b[0m, in \u001b[0;36mGraph.op_def_for_type\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m   3061\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op_def_cache[\u001b[38;5;28mtype\u001b[39m]\n\u001b[0;32m   3062\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m   3063\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op_def_cache[\u001b[38;5;28mtype\u001b[39m] \u001b[38;5;241m=\u001b[39m op_def_pb2\u001b[38;5;241m.\u001b[39mOpDef\u001b[38;5;241m.\u001b[39mFromString(\n\u001b[1;32m-> 3064\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op_def_for_type(\u001b[38;5;28mtype\u001b[39m)\n\u001b[0;32m   3065\u001b[0m   )\n\u001b[0;32m   3066\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op_def_cache[\u001b[38;5;28mtype\u001b[39m]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Op type not registered 'CaseFoldUTF8' in binary running on AJITS-PC. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib (e.g. `tf.contrib.resampler`), accessing should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed."
     ]
    }
   ],
   "source": [
    "# Download BERT preprocessing and encoder model from tensorflow hub\n",
    "preprocessor = hub.KerasLayer(\n",
    "    \"https://kaggle.com/models/tensorflow/bert/TensorFlow2/en-uncased-preprocess/3\")\n",
    "encoder = hub.KerasLayer(\n",
    "    \"https://www.kaggle.com/models/tensorflow/bert/TensorFlow2/bert-en-uncased-l-12-h-768-a-12/2\",\n",
    "    trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca49c2d-d4c9-4e41-a50d-48677e823bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
